# Awesome-MoE-RL

- [Stack (Papers to Review)](#stack-papers-to-review)
- [MoE Papers](#moe)
  - [Core](#core)
  - [Middle](#middle)
  - [Outer](#outer)
- [RL Papers](#rl)
  - [Core](#core-1)
  - [Middle](#middle-1)
  - [Outer](#outer-1)


## Stack (Papers to Review)
Continual Learning via Sparse Memory Finetuning

Multilingual Routing in Mixture-of-Experts

Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR

Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts

Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization

Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum

Reasoning with Sampling: Your Base Model is Smarter Than You Think

MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention

The Art of Scaling Reinforcement Learning Compute for LLMs

DR.LLM: DYNAMIC LAYER ROUTING IN LLMS

Training-Free Group Relative Policy Optimization

How Reinforcement Learning After Next-Token Prediction Facilitates Learning

From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL by Composing Old Ones

Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models

What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?

Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning

Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check

Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?

ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models

7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Efficient

Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective

Learning without training: The implicit dynamics of in-context learning

Mirage or Method? How Model-Task Alignment Induces Divergent RL Conclusions

Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning

Recipes for Pre-training LLMs with MXFP8

## MoE 

### Core

DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model

OLMoE: Open Mixture-of-Experts Language Models

FLEXOLMO: Open Language Models for Flexible Data Use

### Middle

Multilingual Routing in Mixture-of-Experts

### Outer


## RL

### Core

(Project Related) Reinforcement Learning Finetunes Small Subnetworks in Large Language Models

### Middle

### Outer
