# Awesome-MoE-RL

- [Stack (Papers to Review)](#stack-papers-to-review)
- [MoE Papers](#moe)
  - [Core](#core)
  - [Middle](#middle)
  - [Outer](#outer)
- [RL Papers](#rl)
  - [Core](#core-1)
  - [Middle](#middle-1)
  - [Outer](#outer-1)


## Stack (Papers to Review)
Training-Free Group Relative Policy Optimization

How Reinforcement Learning After Next-Token Prediction Facilitates Learning

From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL by Composing Old Ones

Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models

What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?

Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning

Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check

Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?

ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models

7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Efficient

Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective

Learning without training: The implicit dynamics of in-context learning

Mirage or Method? How Model-Task Alignment Induces Divergent RL Conclusions

Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning

## MoE 

### Core

DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model

OLMoE: Open Mixture-of-Experts Language Models

FLEXOLMO: Open Language Models for Flexible Data Use

### Middle

### Outer


## RL

### Core

### Middle

### Outer
